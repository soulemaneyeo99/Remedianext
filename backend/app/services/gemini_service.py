"""
Service Gemini - Interface avec Google Gemini AI

Gestion professionnelle:
- Retry automatique sur erreurs temporaires
- Rate limiting
- Logging d√©taill√©
- Error handling gracieux
"""

import google.generativeai as genai
from typing import Optional, Dict, Any
import logging
from functools import lru_cache
import asyncio
from app.core.config import settings

logger = logging.getLogger(__name__)

class GeminiService:
    """
    Service wrapper pour Google Gemini AI
    
    Features:
    - Configuration centralis√©e
    - Retry logic
    - Error handling
    - Caching (pour prompts syst√®me)
    """
    
    def __init__(self):
        """Initialize Gemini service"""
        self.api_key = settings.gemini_api_key
        self.model_name = settings.gemini_model
        self._model = None
        self._configured = False
        
        if self.api_key:
            self._configure()
    
    def _configure(self):
        """Configure Gemini API"""
        try:
            genai.configure(api_key=self.api_key)
            self._model = genai.GenerativeModel(self.model_name)
            self._configured = True
            logger.info(f"‚úÖ Gemini configured with model: {self.model_name}")
        except Exception as e:
            logger.error(f"‚ùå Gemini configuration failed: {str(e)}")
            self._configured = False
    
    async def chat_medical(self, prompt: str, max_retries: int = 3) -> str:
        """
        Chat m√©dical avec Gemini
        
        Args:
            prompt: Question ou contexte utilisateur
            max_retries: Nombre de tentatives en cas d'erreur
            
        Returns:
            str: R√©ponse textuelle de Gemini
            
        Raises:
            Exception: Si toutes les tentatives √©chouent
        """
        if not self._configured:
            error_msg = "Gemini API non configur√©e. V√©rifier GEMINI_API_KEY."
            logger.error(f"‚ùå {error_msg}")
            raise ValueError(error_msg)
        
        # Prompt syst√®me pour contexte m√©dical
        system_prompt = """Tu es un assistant m√©dical expert en plantes m√©dicinales africaines.

R√àGLES IMPORTANTES:
1. R√©ponds TOUJOURS en fran√ßais
2. Sois professionnel mais empathique
3. Utilise des emojis pour la lisibilit√© (üåø üíä ‚ö†Ô∏è ‚úÖ)
4. Structure tes r√©ponses avec des listes √† puces
5. Cite TOUJOURS les sources scientifiques si disponibles
6. Mentionne TOUJOURS les pr√©cautions d'usage
7. Si c'est une urgence m√©dicale, recommande de consulter un professionnel
8. Base-toi sur des connaissances valid√©es scientifiquement

CONTEXTE:
Tu travailles pour REM√âDIA, une plateforme qui d√©mocratise l'acc√®s aux plantes m√©dicinales africaines en combinant savoirs traditionnels et validation scientifique.

R√âPONDEZ MAINTENANT:"""
        
        # Combiner syst√®me + user prompt
        full_prompt = f"{system_prompt}\n\n{prompt}"
        
        # Retry loop
        for attempt in range(max_retries):
            try:
                logger.info(f"ü§ñ Calling Gemini API (attempt {attempt + 1}/{max_retries})")
                
                # G√©n√©rer r√©ponse (sync call dans async context)
                response = await asyncio.to_thread(
                    self._model.generate_content,
                    full_prompt
                )
                
                # Extraire texte de la r√©ponse
                if hasattr(response, 'text'):
                    response_text = response.text
                elif hasattr(response, 'parts'):
                    # Si r√©ponse en parties, combiner
                    response_text = ' '.join([part.text for part in response.parts if hasattr(part, 'text')])
                else:
                    # Fallback: convertir en string
                    response_text = str(response)
                
                logger.info(f"‚úÖ Gemini response received ({len(response_text)} chars)")
                
                # CRITIQUE: Retourner STRING, pas dict
                return response_text
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Gemini attempt {attempt + 1} failed: {str(e)}")
                
                if attempt < max_retries - 1:
                    # Attendre avant retry (exponential backoff)
                    wait_time = 2 ** attempt  # 1s, 2s, 4s
                    logger.info(f"‚è≥ Retrying in {wait_time}s...")
                    await asyncio.sleep(wait_time)
                else:
                    # Toutes tentatives √©chou√©es
                    logger.error(f"‚ùå All Gemini attempts failed: {str(e)}")
                    raise Exception(f"Erreur Gemini apr√®s {max_retries} tentatives: {str(e)}")
    
    async def identify_plant(self, image_data: bytes, prompt: Optional[str] = None) -> str:
        """
        Identifier une plante depuis une image
        
        Args:
            image_data: Bytes de l'image
            prompt: Prompt additionnel optionnel
            
        Returns:
            str: Identification de la plante
        """
        if not self._configured:
            raise ValueError("Gemini API non configur√©e")
        
        try:
            # Prompt syst√®me pour identification
            system_prompt = """Tu es un expert botaniste sp√©cialis√© dans les plantes m√©dicinales africaines.

MISSION:
Identifie la plante dans cette image et fournis:
1. Nom scientifique (latin)
2. Noms communs (fran√ßais + locaux africains)
3. Famille botanique
4. Usages m√©dicinaux traditionnels
5. Propri√©t√©s m√©dicinales valid√©es scientifiquement
6. Pr√©paration recommand√©e
7. Dosage s√ªr
8. Pr√©cautions et contre-indications

FORMAT:
Utilise emojis (üåø pour le nom, üíä pour usages, ‚ö†Ô∏è pour pr√©cautions).
Structure en sections claires.

IMPORTANT:
- Si l'identification est incertaine, le dire clairement
- Si la plante est toxique, l'indiquer en GRAS
- Toujours mentionner de consulter un expert en cas de doute

R√âPONDEZ MAINTENANT:"""
            
            full_prompt = system_prompt
            if prompt:
                full_prompt += f"\n\nContexte additionnel: {prompt}"
            
            # Pr√©parer image pour Gemini
            from PIL import Image
            import io
            
            image = Image.open(io.BytesIO(image_data))
            
            # G√©n√©rer avec image
            logger.info("üîç Identifying plant with Gemini Vision...")
            response = await asyncio.to_thread(
                self._model.generate_content,
                [full_prompt, image]
            )
            
            # Extraire texte
            if hasattr(response, 'text'):
                response_text = response.text
            else:
                response_text = str(response)
            
            logger.info(f"‚úÖ Plant identified ({len(response_text)} chars)")
            
            # CRITIQUE: Retourner STRING
            return response_text
            
        except Exception as e:
            logger.error(f"‚ùå Plant identification failed: {str(e)}")
            raise Exception(f"Erreur identification plante: {str(e)}")
    
    @lru_cache(maxsize=100)
    def get_cached_plant_info(self, plant_name: str) -> Optional[Dict[str, Any]]:
        """
        Cache pour infos plantes fr√©quemment demand√©es
        
        Note: Utilise lru_cache pour √©viter appels API r√©p√©t√©s
        """
        # TODO: Impl√©menter cache Redis en production
        return None


# Singleton instance
gemini_service = GeminiService()


# Export pour imports directs
__all__ = ['gemini_service', 'GeminiService']